# Machine Learning In Production 
### notes from course, other sources and my inference from other projects
- [ML Lifecycle](#mllifecycle)
- ML Project [Deployment Patterns](#deploypatterns)
- ML Project [Cycles - Building and After Deployment](#cycles)
- ML project [Drift - After Deployment](#drift) 
- ML project [Metrics - After Deployment](#metrics) 

## MLLifeCycle
[ML LifeCycle Diagram](./MLLifeCycle.png) </br>
[LandingAI_Platform_WhitePaper](https://landing.ai/wp-content/uploads/2021/06/LandingAI_Platform_WhitePaper.pdf) </br>
- Identify the key components 
  - ML System = Code + Model (algo + param + traindata features) + Data 
  - Ownership of code /data in time and place changes. Service does not own customer data.
    - [ ] Scope the Project
     - f(x) = y 
    - [ ] Get Data 
      - Acquire x
      - Labeling y
    - [ ] **Build Model** 
      - *Iterative Process* (Tune, train data, params, model)  
      - Build - f(x) - Research find the right architecture
      - Hyper Parameters 
      - Train
      - Error Analysis 
        - Classification, Regression, Generation
    - [ ] Deploy 
      - *Iterative Process* (Tune - metrics monitored, percentage served with model) - This is the BEGINNING
      - Test Deployed
      - Deploy
    - [ ] Monitor And Maintain the System
      - gather production data, monitor data, analyze 
      - **Update the Model** continuously - Production - Architecture/Algorithm is fixed, Data HyperParameters change 

## Cycles 
- Compare and contrast the ML modeling iterative cycle with the cycle for deployment of ML products.
  - Build and update data based on Error Analysis
  - **[Build Model](#BuildModel) - Deployment ready**
  - **[Update Deployed Model](#UpdateModel)**
    
## DeployPatterns
- Deployment scenarios in the context of varying degrees of automation.
  - shadow 
    - when there is a human doing the same task
    - deploy in parallel to human task to capture the labeled data and validate with human label.
  - canary
    - route 5% request to the model and rest to human
    - ensure the system/model works before switching ot use for more requests 
  - blue-green
    - When deploying V2(green), keep the V1)(blue) in deployment environment to revert immediately.
  
## Drift
[Reference: Concept Data Drift](https://towardsdatascience.com/machine-learning-in-production-why-you-should-care-about-data-and-concept-drift-d96d0bc907fb)
  - Data Drift - change in data distribution
    - Data drift, feature drift, population, or covariate shift. 
    - Mean and Variance shift of data columns.
  - Concept Drift - change in data relationship 
    - Model Decay
    - Incoming data varies significantly from trained data
    - Completely new data as opposed to diff data as in data drift 
  - Example/Reference Tools:
    - [OpenSource Model and Data Monitoring tools](https://github.com/evidentlyai/evidently)

## Metrics
- Monitoring the deployed system </br>
[Reference: Data/Concept(Model) Drift](https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/) </br>
[Reference: Data vs Model Centric in production](https://www.youtube.com/watch?v=06-AZXmwHjo)  </br>
  - Dashboard - List the typical metrics you might track to monitor concept drift.
  - Software Metrics - Server Load  
    - Memory
    - CPU
    - Latency
    - Through put
    - Availability
    - Reliability
  - Model Metrics
    - Input Data 
      - Structured Data
        - Non null outputs
        - number of in-coming columns
      - UnStructured Data - thresholds
        - Image brightness, size, resolution
        - Audio . video stream length
        - How much of clip processed is empty
    - Output Data
      - Exceptional processing usage when model did not help user

##BuildModel
  - Data Centric
    - Detect Type of data 
    - Decide on Model Architecture
    - Training - iterative experiments- data , model , hyperparameters update to get best fit
    - Performance, Accuracy(Precision/Recall), Error Tests on Business Metrics
    - Data - analysis 
      - balance
      - less data for one type - rare classes 
    - Business Metrics
      - *Software 1.0 - it works on my computer // Software 2.0 - it works on my test set* 
      - Good enough, approximate results - web search
      - Looking for exact task oriented results - getting a false neg increases time vs. false positive
      - class accuracies biases for certain type of data - showing one vedor (based on products sold prior only), one type of people
    - Establish BaseLine Perf
      - unstructured data          - Human Level Perf 
      - structured data(tabular)   - State of Art / open source
      - Existing system or process perf 
  
##UpdateModel
