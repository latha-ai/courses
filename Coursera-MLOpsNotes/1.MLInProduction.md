# Machine Learning In Production 
### notes from course, other sources and my inference from other projects
- [ML Lifecycle](#mllifecycle)
- ML Project [Deployment Patterns](#deploypatterns)
- ML Project [Cycles - Building and After Deployment](#cycles)
- ML project [Drift - After Deployment](#drift) 
- ML project [Metrics - After Deployment](#metrics) 

## MLLifeCycle
[ML LifeCycle Diagram](./MLLifeCycle.png) </br>
[LandingAI_Platform_WhitePaper](https://landing.ai/wp-content/uploads/2021/06/LandingAI_Platform_WhitePaper.pdf) </br>
- Identify the key components 
  - ML System = Code + Model (algo + param + traindata features) + Data 
  - Ownership of code /data in time and place changes. Service does not own customer data.
    - [ ] [Scope](#scoping) the Project
     - f(x) = y 
    - [ ] Get [Data](#data) 
      - Acquire x
      - Labeling y
    - [ ] **Build Model** 
      - *Iterative Process* (Tune, train data, params, model)  
      - Build - f(x) - Research find the right architecture
      - Hyper Parameters 
      - Train
      - [Error Analysis](#erroranalysis)
      - [Experiment Tracking](#experimenttracking)
    - [ ] Deploy 
      - *Iterative Process* (Tune - metrics monitored, percentage served with model) - This is the BEGINNING
      - Test Deployed
      - Deploy
    - [ ] [Monitor](#monitor) And Maintain the System
      - gather production data, monitor data, analyze 
      - **Update the Model** continuously - Production - Architecture/Algorithm is fixed, Data HyperParameters change 

## Scoping
- ML for ML
- Identify business problem and goal, map the technology and solution
- Economic value 
  - automating the decision-making process - what is complemented, supplemented 
  - new avenues of automated process 
    - Production env changes to make the model effective - model does not directly solve the business problem 
      - Capturing good data upstream
      - Changing the process to capture the feedback  
- Feasibility
- Diligence
- Ethical - bias / value
- Governance
- Regulations
- Milestones + Resources = [Metrics](#metrics)

## Cycles 
- Compare and contrast the ML modeling iterative cycle with the cycle for deployment of ML products.
  - Build and update data based on Error Analysis
  - **[Build Model](#BuildModel) - Deployment ready**
  - **[Update Deployed Model](#UpdateModel)**
    
## DeployPatterns
- Deployment scenarios in the context of varying degrees of automation.
  - shadow 
    - when there is a human doing the same task
    - deploy in parallel to human task to capture the labeled data and validate with human label.
  - canary
    - route 5% request to the model and rest to human
    - ensure the system/model works before switching to use for more requests 
  - blue-green
    - When deploying V2(green), keep the V1)(blue) in deployment environment to revert immediately.
  
## BuildModel
- Data Centric
  - Detect Type of data 
  - Decide on Model Architecture
  - Training - iterative experiments- data , model , hyperparameters update to get best fit
  - Performance, Accuracy(Precision/Recall), Error Tests on Business Metrics
- Data - analysis 
  - balance
  - less data for one type - rare classes 
- Business Metrics
  - *Software 1.0 - it works on my computer // Software 2.0 - it works on my test set* 
  - Good enough, approximate results - web search
  - Looking for exact task oriented results - getting a false neg increases time vs. false positive
  - class accuracies biases for certain type of data - showing one vedor (based on products sold prior only), one type of people
- Establish BaseLine Perf
  - deployment compute constraints
  - unstructured data          - Human Level Perf 
  - structured data(tabular)   - State of Art / open source
  - Existing system or process perf 
- [ErrorAnalysis](#erroranalysis)
  - Iterate with fixed data for categories in error
  - data augmentation

## ErrorAnalysis
- manual
  - track attributes that are in error
    - source of errors - noise or main features
- automated

## Data
- Skewed, not balanced dataset
  - HLP only for baseline
  - measure production, analyze and compare with HLP to remove the HLP subjective data
  - lable consistency
  - standardization
  - merging - de-duplication
- Training
  - start with min data - add incrementally
  - train/dev/test split - random vs. balanced split
- Structured vs Unstructured
- Small vs Big 

## ExperimentTracking
- Tools Comet, WeightsAndBiases, mlflow, SageMaker, MetaFlow
  - Track data source, steps of processing data

## UpdateModel
- Update model in production

## Monitor
- Regulations
  - Training 
  - Runtime
- ML systems: [GCP](https://cloud.google.com/blog/products/ai-machine-learning/key-requirements-for-an-mlops-foundation)
  - Continuous Integration (CI) 
    - is not only about testing and validating code and components, but also testing and validating data, data schemas, and models.
  - Continuous Delivery (CD) is not only about a single software package or a service,
    - but a system (an ML training pipeline) that should automatically deploy another service (model prediction service).
  Continuous Training (CT) is a new property, unique to ML systems, 
    - that's concerned with automatically retraining candidate models for testing and serving.
  Continuous Monitoring (CM) is not only about catching errors in production systems, 
    - but also about monitoring production inference data and model performance metrics tied to business outcomes.

## Drift
  - Data Drift - change in data distribution
    - Data drift, feature drift, population, or covariate shift. 
    - Mean and Variance shift of data columns.
  - Concept Drift - change in data relationship 
    - Model Decay
    - Incoming data varies significantly from trained data
    - Completely new data as opposed to diff data as in data drift 

## Metrics
- Monitoring the deployed system </br>
  - Dashboard - List the typical metrics you might track to monitor concept drift.
  - Software Metrics - Server Load  
    - Memory
    - CPU
    - Latency
    - Through put
    - Availability
    - Reliability
  - Model Metrics
    - Input Data 
      - Structured Data
        - Non null outputs
        - number of in-coming columns
      - UnStructured Data - thresholds
        - Image brightness, size, resolution
        - Audio . video stream length
        - How much of clip processed is empty
    - Output Data
      - Exception processing usage when model did not help user
